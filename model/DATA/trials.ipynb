{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac88880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12da4b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcsk_GyEyw_5S6CquThjRFfj1uT7jEu3kGh73EAzb4LokD1MLEZEKQHXNsstgWYzEDSUiwu3jm\n"
     ]
    }
   ],
   "source": [
    "print(\"pcsk_GyEyw_5S6CquThjRFfj1uT7jEu3kGh73EAzb4LokD1MLEZEKQHXNsstgWYzEDSUiwu3jm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1284b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = \"pcsk_GyEyw_5S6CquThjRFfj1uT7jEu3kGh73EAzb4LokD1MLEZEKQHXNsstgWYzEDSUiwu3jm\"\n",
    "\n",
    "PINECONE_API_ENV = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cfb2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e732517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202967eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 100, chunk_overlap = 200)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d0f63bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 907/907 [03:02<00:00,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI at the Edge\n",
      "Solving Real-World Problems with\n",
      "Embedded Machine Learning\n",
      "Compliments of\n",
      "DDaanniieell SSiittuunnaayyaakkee\n",
      "&& JJeennnnyy PPlluunnkkeetttt\n",
      "FFoorreewwoorrdd bbyy PPeettee WWaarrddeenn\n",
      "AI at the Edge\n",
      "Edge AI is transforming the way computers interact with the\n",
      "real world, allowing IoT devices to make decisions using the\n",
      "‚ÄúAI at the Edge provides\n",
      "99% of sensor data that was previously discarded due to\n",
      "an excellent introduction\n",
      "cost, bandwidth, or power limitations. With techniques like\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF using pdfplumber, without skipping any pages, and show progress.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf.pages)\n",
    "        for i in tqdm(range(total_pages), desc=\"Extracting text\"):\n",
    "            page_text = pdf.pages[i].extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Extract text\n",
    "extracted_data = load_pdf(r\"C:\\Users\\mohda\\OneDrive\\Desktop\\PROJECTS\\leo\\model\\DATA\\leo data set.pdf\")\n",
    "print(extracted_data[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a16961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1795e0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 9374 text chunks from PDF.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_text(extracted_data)\n",
    "\n",
    "print(f\"Extracted {len(text_chunks)} text chunks from PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b481178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3b92255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd8da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohda\\AppData\\Local\\Temp\\ipykernel_25848\\4238859041.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\mohda\\anaconda3\\envs\\leo\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7b7f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb8ac312",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1725a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea522c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embedding Vector: [0.05694784224033356, -0.03384576365351677, -0.03826654702425003, 0.029937393963336945, -0.0020012466702610254, -0.01088137086480856, 0.05203469470143318, 0.004358153790235519, -0.03935890272259712, -0.014833830296993256]\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "text = \"what are top ten places to visit in inida?\"\n",
    "query_embedding = embedding_model.embed_query(text)\n",
    "\n",
    "print(\"Generated Embedding Vector:\", query_embedding[:10])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d1a84f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6809d117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa7cef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_GyEyw_5S6CquThjRFfj1uT7jEu3kGh73EAzb4LokD1MLEZEKQHXNsstgWYzEDSUiwu3jm\")\n",
    "index = pc.Index(\"leo\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55c66b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2e7a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz #install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f624aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "pdf_path = r\"C:\\Users\\mohda\\OneDrive\\Desktop\\PROJECTS\\leo\\model\\DATA\\leo data set.pdf\"  # Replace with the actual path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb4e3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = extract_text_from_pdf(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f25b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 3115 text chunks from PDF.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "text_chunks = text_splitter.split_text(extracted_text)\n",
    "print(f\"‚úÖ Extracted {len(text_chunks)} text chunks from PDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5139ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to index: leo\n",
      "Upserted 100 vectors so far.\n",
      "Upserted 200 vectors so far.\n",
      "Upserted 300 vectors so far.\n",
      "Upserted 400 vectors so far.\n",
      "Upserted 500 vectors so far.\n",
      "Upserted 600 vectors so far.\n",
      "Upserted 700 vectors so far.\n",
      "Upserted 800 vectors so far.\n",
      "Upserted 900 vectors so far.\n",
      "Upserted 1000 vectors so far.\n",
      "Upserted 1100 vectors so far.\n",
      "Upserted 1200 vectors so far.\n",
      "Upserted 1300 vectors so far.\n",
      "Upserted 1400 vectors so far.\n",
      "Upserted 1500 vectors so far.\n",
      "Upserted 1600 vectors so far.\n",
      "Upserted 1700 vectors so far.\n",
      "Upserted 1800 vectors so far.\n",
      "Upserted 1900 vectors so far.\n",
      "Upserted 2000 vectors so far.\n",
      "Upserted 2100 vectors so far.\n",
      "Upserted 2200 vectors so far.\n",
      "Upserted 2300 vectors so far.\n",
      "Upserted 2400 vectors so far.\n",
      "Upserted 2500 vectors so far.\n",
      "Upserted 2600 vectors so far.\n",
      "Upserted 2700 vectors so far.\n",
      "Upserted 2800 vectors so far.\n",
      "Upserted 2900 vectors so far.\n",
      "Upserted 3000 vectors so far.\n",
      "Upserted 3100 vectors so far.\n",
      "Upserted 3115 vectors so far.\n",
      "‚úÖ PDF text chunks successfully converted to vectors and stored in Pinecone!\n"
     ]
    }
   ],
   "source": [
    "# üîπ Step 3: Initialize Pinecone Client\n",
    "pc = Pinecone(api_key=\"pcsk_GyEyw_5S6CquThjRFfj1uT7jEu3kGh73EAzb4LokD1MLEZEKQHXNsstgWYzEDSUiwu3jm\")  # Replace with your actual API Key\n",
    "\n",
    "# Define Pinecone index details\n",
    "index_name = \"leo\"\n",
    "expected_dim = 384  # Ensure embeddings match this dimension\n",
    "\n",
    "\n",
    "\n",
    "# Connect to the existing index\n",
    "index = pc.Index(index_name)\n",
    "print(f\"Connected to index: {index_name}\")\n",
    "\n",
    "# üîπ Step 4: Convert Text Chunks into Vector Embeddings\n",
    "embeddings = embedding_model.embed_documents(text_chunks)\n",
    "\n",
    "# üîπ Step 5: Format Data for Pinecone Upsert\n",
    "vectors = [\n",
    "    {\"id\": f\"doc_{i}\", \"values\": embeddings[i], \"metadata\": {\"text\": text_chunks[i]}}\n",
    "    for i in range(len(text_chunks))\n",
    "]\n",
    "\n",
    "# üîπ Step 6: Store Vectors in Pinecone\n",
    "batch_size = 100  # To prevent exceeding Pinecone's request limits\n",
    "for i in range(0, len(vectors), batch_size):\n",
    "    index.upsert(vectors=vectors[i : i + batch_size])\n",
    "    print(f\"Upserted {i + len(vectors[i : i + batch_size])} vectors so far.\")\n",
    "\n",
    "print(\"‚úÖ PDF text chunks successfully converted to vectors and stored in Pinecone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06136fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_url = \"https://leo-kvk4kva.svc.aped-4627-b74a.pinecone.io\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8ca8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_response(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e111fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Query Results:\n",
      "üîπ Confidence Score: 0.4023\n",
      "üìÑ Retrieved Text: filtering, 90\n",
      "Finlayson‚Äôs squirrel (Callosciurus finlaysonii),\n",
      "366, 373\n",
      "FIRST (fair, inclusive, responsible, safe, trans‚Äê\n",
      "parent) checklist, 446\n",
      "first responders, intelligent wearables for, 29\n",
      "fitness tracker, 8-10\n",
      "flash memory, 70\n",
      "flex sensors, 64\n",
      "floating-point operations per second (FLOPS),\n",
      "332\n",
      "floating-point unit (FPU), 69\n",
      "flow sensors, 64\n",
      "FNR (false negative rate), 326\n",
      "FOMO (Faster Objects, More Objects), 293\n",
      "food quality assurance use case, 407-440\n",
      "bootstrapping, 414\n",
      "data collection, 417\n",
      "data ingestion firmware, 417\n",
      "dataset cleaning, 420-421\n",
      "dataset gathering, 415-423\n",
      "dataset licensing/legal obligations, 422\n",
      "defining machine learning classes, 414\n",
      "deployment, 432-436\n",
      "design considerations, 412\n",
      "DSP approach, 423-425\n",
      "DSP block, 424\n",
      "Edge Impulse and, 415\n",
      "environmental/social impact, 413\n",
      "\n",
      "üîπ Confidence Score: 0.3254\n",
      "üìÑ Retrieved Text: ficient data, 41, 46, 207\n",
      "semi-supervised learning, 237-240, 245\n",
      "semiconductor detectors, 65\n",
      "sensor data, as focus of edge AI, 20\n",
      "sensor fusion, 93-94, 416\n",
      "sensor fusion flow, 285\n",
      "sensors\n",
      "acoustic and vibration, 59\n",
      "basics, 55-58\n",
      "bicyclist monitor use case, 449\n",
      "choices for data logging, 305\n",
      "combining features and sensors, 93-95\n",
      "consumer products, 445\n",
      "difficulties of data capture at the edge, 218\n",
      "environmental, biological, chemical, 65\n",
      "food quality assurance, 412\n",
      "force and tactile, 63\n",
      "motion and position, 62\n",
      "optical, electromagnetic, radiation, 64\n",
      "types of, 58\n",
      "visual and scene, 60-62\n",
      "wildlife monitoring, 363\n",
      "sequence models, 109\n",
      "sequential binary data format, 248\n",
      "series flow, 283\n",
      "services, architectural design and, 279\n",
      "shipping, smart packaging and, 33\n",
      "signals\n",
      "transforming, 39-41\n",
      "\n",
      "üîπ Confidence Score: 0.3154\n",
      "üìÑ Retrieved Text: feature engineering, 85-95\n",
      "combining features and sensors, 93-95\n",
      "data preparation and, 255\n",
      "digital signal processing algorithms, 88-93\n",
      "working with data streams, 86-88\n",
      "feature extractors, 113\n",
      "feature scaling, 95\n",
      "features (dataset element), 201, 235\n",
      "federated learning\n",
      "on-device training, 121\n",
      "TensorFlow Federated, 157\n",
      "feedback\n",
      "edge AI limitations, 21\n",
      "solving problems with, 350\n",
      "types of feedback from deployed systems,\n",
      "344-349\n",
      "feedback loops, 307\n",
      "(see also testing and iteration)\n",
      "bicyclist monitor use case, 467\n",
      "in development process, 307-309\n",
      "food quality assurance use case, 437-439\n",
      "performance calibration, 337\n",
      "wildlife monitoring use case, 401-403\n",
      "field of view, 60\n",
      "field programmable gate arrays (FPGAs), 78\n",
      "filtering, 90\n",
      "Finlayson‚Äôs squirrel (Callosciurus finlaysonii),\n",
      "366, 373\n",
      "\n",
      "\n",
      "ü§ñ Answer: no\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "query = \"hoe many fingures do you have?\" \n",
    "query_embedding = embedding_model.embed_query(query)\n",
    "\n",
    "query_result = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=3,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Query Results:\")\n",
    "for match in query_result[\"matches\"]:\n",
    "    score = match[\"score\"]\n",
    "    text = match[\"metadata\"].get(\"text\", \"‚ö† No matching text found\")\n",
    "    print(f\"üîπ Confidence Score: {score:.4f}\")\n",
    "    print(f\"üìÑ Retrieved Text: {text}\\n\")\n",
    "\n",
    "# Step 1: Load the LLM (FLAN-T5 Base is lightweight and free)\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Step 2: Combine Retrieved Text Chunks into a Single Context\n",
    "retrieved_text = \"\\n\\n\".join([\n",
    "    match[\"metadata\"].get(\"text\", \"\") \n",
    "    for match in query_result[\"matches\"]\n",
    "])\n",
    "\n",
    "# Optional: Trim if it's too long for the model\n",
    "retrieved_text = retrieved_text[:1500]  # Tokens, not characters\n",
    "\n",
    "# Step 3: Construct Prompt\n",
    "prompt = f\"\"\"\n",
    "you are a helpful assistant. review the data retrieved and frame a good answer\n",
    "\n",
    "Context:\n",
    "{retrieved_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "# Step 4: Generate Response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nü§ñ Answer:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90945c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
